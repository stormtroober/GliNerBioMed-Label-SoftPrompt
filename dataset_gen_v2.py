# -*- coding: utf-8 -*-
"""
Generazione dataset token-level BIO-aware con mascherizzazione dei subtoken
per GLiNER-BioMed (bi-encoder token-level training).

Pipeline:
1Ô∏è‚É£ Legge il dataset JNLPBA (train o test)
2Ô∏è‚É£ Mantiene i tag BIO (B- / I- / O)
3Ô∏è‚É£ Tokenizza e allinea le label a livello di subtoken
    - B-XXX ‚Üí label valida solo sul primo subtoken
    - I-XXX ‚Üí label propagata anche ai subtoken interni
    - O     ‚Üí fuori da entit√†
4Ô∏è‚É£ Bilancia il dataset
5Ô∏è‚É£ Esporta in JSON leggibile
6Ô∏è‚É£ Verifica automatica finale
"""

import pandas as pd
from tqdm import tqdm
import json
import torch
from transformers import AutoTokenizer
from collections import Counter, defaultdict
import random

# ===============================================================
# 1Ô∏è‚É£ CONFIGURAZIONE
# ===============================================================
print("üì• Caricamento label2id.json e tokenizer...")
with open("label2id.json") as f:
    label2id = json.load(f)
id2label = {v: k for k, v in label2id.items()}

tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-small")

# Dataset remoto HuggingFace (JNLPBA)
splits = {
    "train": "data/train-00000-of-00001.parquet",
    "test": "data/test-00000-of-00001.parquet"
}
df_data = pd.read_parquet("hf://datasets/disi-unibo-nlp/JNLPBA/" + splits["train"])
df = df_data.head(15000).copy()  # subset di test rapido
print(f"‚úÖ Dataset caricato: {len(df)} righe")

# ===============================================================
# 2Ô∏è‚É£ MAPPATURA BIO ‚Üí LABEL BASE
# ===============================================================
BIO2BASE = {
    "DNA": "dna",
    "PROTEIN": "protein",
    "CELL_TYPE": "cell type",
    "CELL LINE": "cell line",
    "CELL_LINE": "cell line",
    "RNA": "rna",
}

def parse_bio_tag(tag: str):
    """Ritorna (prefisso, label_base) ‚Äî gestisce anche formati irregolari."""
    if tag == "O":
        return ("O", "O")
    pref, _, typ = tag.partition("-")
    base = BIO2BASE.get(typ.upper().replace("-", " "), typ.lower())
    return (pref, base or "O")

# ===============================================================
# 3Ô∏è‚É£ TOKENIZZAZIONE + MASCHERIZZAZIONE (BIO-AWARE)
# ===============================================================
def encode_and_align_labels(words, bio_tags, tokenizer, label2id):
    """
    Tokenizza e allinea le label a livello di subtoken.
    BIO-aware:
      - B-XXX ‚Üí label sul primo subtoken
      - I-XXX ‚Üí label sulle parole successive dello stesso tipo
      - Subtoken interni (della stessa parola) ‚Üí -100
    """
    bio_info = [parse_bio_tag(t) for t in bio_tags]
    prefixes, base_labels = zip(*bio_info)

    enc = tokenizer(
        words,
        is_split_into_words=True,
        return_tensors="pt",
        truncation=True
    )
    word_ids = enc.word_ids(0)
    labels_subtok = []
    previous_word_idx = None

    for word_idx in word_ids:
        if word_idx is None:
            labels_subtok.append(-100)
            continue

        pref, lbl = prefixes[word_idx], base_labels[word_idx]

        # Se √® il primo subtoken della parola
        if word_idx != previous_word_idx:
            if pref in ["B", "I"] and lbl != "O":
                labels_subtok.append(label2id.get(lbl, label2id["O"]))
            else:
                labels_subtok.append(label2id["O"])
        else:
            # ‚ö†Ô∏è Subtoken interno della stessa parola ‚Üí sempre -100
            labels_subtok.append(-100)

        previous_word_idx = word_idx

    enc = {k: v.squeeze(0) for k, v in enc.items()}
    enc["labels"] = torch.tensor(labels_subtok)
    return enc


# ===============================================================
# 4Ô∏è‚É£ COSTRUZIONE DEL DATASET
# ===============================================================
print("\n‚öôÔ∏è  Costruzione dataset token-level BIO-aware...")

encoded_dataset = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    words = list(row["tokens"])
    bio_tags = list(row["ner_tags"])
    if not words or not bio_tags or len(words) != len(bio_tags):
        continue
    ex = encode_and_align_labels(words, bio_tags, tokenizer, label2id)
    encoded_dataset.append(ex)

print(f"\n‚úÖ Creati {len(encoded_dataset)} esempi token-level.")

# ===============================================================
# 5Ô∏è‚É£ BILANCIAMENTO CLASSI
# ===============================================================
label_counts = Counter()
for ex in encoded_dataset:
    for l in ex["labels"].tolist():
        if l != -100:
            label_counts[l] += 1

non_o_labels = [lid for lid in label_counts if id2label[lid] != "O"]
min_count = 100
balanced_examples = defaultdict(list)

for ex in encoded_dataset:
    labels = ex["labels"].tolist()
    for lid in set([l for l in labels if l != -100 and id2label[l] != "O"]):
        balanced_examples[lid].append(ex)

balanced_dataset = []
for lid in non_o_labels:
    random.shuffle(balanced_examples[lid])
    balanced_dataset.extend(balanced_examples[lid][:min_count])

print(f"\n‚úÖ Dataset bilanciato con {len(balanced_dataset)} frasi ({min_count} per classe)")

# ===============================================================
# 6Ô∏è‚É£ ESPORTAZIONE IN JSON
# ===============================================================
records = []
for i, ex in enumerate(balanced_dataset):
    tokens = tokenizer.convert_ids_to_tokens(ex["input_ids"], skip_special_tokens=True)
    labels = [
        id2label.get(l, "IGNORE") if l != -100 else "IGNORE"
        for l in ex["labels"].tolist()
    ]
    records.append({"id": i, "tokens": tokens, "labels": labels})

with open("dataset_masked_balanced_bio.json", "w", encoding="utf-8") as f:
    json.dump(records, f, indent=2, ensure_ascii=False)

print(f"üíæ Salvato in: dataset_masked_balanced_bio.json ({len(records)} esempi)")

# ===============================================================
# 7Ô∏è‚É£ VERIFICA FINALE COMPLETA SU TUTTO IL DATASET
# ===============================================================
def verify_final_dataset(records, tokenizer, max_inconsistency_ratio=0.02):
    """
    Verifica completa dell'intero dataset.
      - controlla coerenza subtoken/label
      - valuta distribuzione mascherizzazione
      - calcola tasso di incongruenze
      - stampa validazione finale automatica
    """
    print(f"\nüß™ Verifica finale BIO-aware su {len(records)} frasi (intero dataset)...")
    total_tokens = 0
    ignored_tokens = 0
    valid_label_tokens = 0
    inconsistencies = []

    for i, rec in enumerate(records):
        tokens = rec["tokens"]
        labels = rec["labels"]

        for tok, lab in zip(tokens, labels):
            total_tokens += 1
            if lab == "IGNORE":
                ignored_tokens += 1
            else:
                valid_label_tokens += 1

            # Subtoken non iniziale con label valida ‚Üí potenziale errore
            if not tok.startswith("‚ñÅ") and lab not in ["IGNORE", "O"]:
                # ignora numeri, acronimi, punteggiatura
                if (
                    any(c.isdigit() for c in tok)
                    or tok.isupper()
                    or len(tok) <= 2
                    or tok in ["-", "/", "(", ")", "‚Äô", "'", '"', ".", ",", ":"]
                ):
                    continue
                inconsistencies.append((i, tok, lab))

    # Statistiche globali
    mask_ratio = ignored_tokens / total_tokens if total_tokens else 0
    valid_ratio = valid_label_tokens / total_tokens if total_tokens else 0
    inconsistency_ratio = len(inconsistencies) / total_tokens if total_tokens else 0

    print("\nüìä STATISTICHE GLOBALI")
    print(f"   ‚Ä¢ Token totali analizzati: {total_tokens}")
    print(f"   ‚Ä¢ Token mascherati (-100 / IGNORE): {ignored_tokens} ({mask_ratio*100:.2f}%)")
    print(f"   ‚Ä¢ Token con label valida: {valid_label_tokens} ({valid_ratio*100:.2f}%)")
    print(f"   ‚Ä¢ Incongruenze trovate: {len(inconsistencies)} ({inconsistency_ratio*100:.3f}%)")

    # Risultato finale
    print("\nüß© RISULTATO FINALE")
    if inconsistency_ratio <= max_inconsistency_ratio:
        print(f"‚úÖ Dataset approvato: incongruenze entro la soglia ({max_inconsistency_ratio*100:.1f}%)")
    else:
        print(f"‚ö†Ô∏è Dataset da rivedere: troppe incongruenze ({inconsistency_ratio*100:.2f}%)")

    # Esempi campione di incongruenze (se presenti)
    if inconsistencies:
        print("\n‚ö†Ô∏è  Prime 10 incongruenze rilevate:")
        for i, tok, lab in inconsistencies[:10]:
            print(f"  ‚Ä¢ Frase {i}: subtoken '{tok}' ‚Üí '{lab}'")

# üîß Esegui la verifica completa
verify_final_dataset(records, tokenizer, max_inconsistency_ratio=0.02)

print("\n‚úÖ Fine generazione e verifica completa dataset BIO-aware.")
